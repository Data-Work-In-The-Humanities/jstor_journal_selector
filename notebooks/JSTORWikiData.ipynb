{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"mimetype\")\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_wiki_global_txt_file = pd.read_csv(\"../data/original_journal_datasets/jstor/cleaned_jstor_titles_inferred_wiki.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get wikidata jstor_id with all properties\n",
    "- get wikidata values for properties with URI\n",
    "- get sitelinks from wikidata\n",
    "- get wikipedia page for sitelinks\n",
    "- scrape wikipedia page for publication in multiple languages https://github.com/martin-majlis/Wikipedia-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import wikidata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a SPARQL query for a batch of JSTOR IDs\n",
    "def create_batch_query(jstor_ids):\n",
    "    # Create a VALUES clause with all the JSTOR IDs\n",
    "    values_clause = ' '.join(f'(\"{jstor_id}\")' for jstor_id in jstor_ids)\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?jstorID ?item ?itemLabel ?article ?publicationInterval WHERE {{\n",
    "      VALUES (?jstorID) {{ {values_clause} }}\n",
    "      ?item wdt:P1230 ?jstorID.\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P2896 ?publicationInterval.\n",
    "        ?article schema:about ?item;\n",
    "                 schema:isPartOf <https://en.wikipedia.org/>.\n",
    "      }}\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return sparql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_ids = [\"amerhistrevi\"]\n",
    "query = create_batch_query(jstor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidata.client import Client\n",
    "\n",
    "def fetch_wikidata(qid):\n",
    "    client = Client()\n",
    "    entity = client.get(qid, load=True)\n",
    "    return entity\n",
    "\n",
    "test_entity = fetch_wikidata('Q389936')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(test_entity.data['sitelinks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitelinks(entity):\n",
    "    sitelinks = entity.data['sitelinks']\n",
    "    sitelinks_df = pd.DataFrame(sitelinks).T\n",
    "    return sitelinks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_query(jstor_ids, property_ids):\n",
    "    # Create a VALUES clause with all the JSTOR IDs\n",
    "    values_clause = ' '.join(f'(\"{jstor_id}\")' for jstor_id in jstor_ids)\n",
    "    # Create a part of the WHERE clause for each property ID\n",
    "    where_clause = ' '.join(f'OPTIONAL {{ ?item wdt:{property_id} ?{property_id} . }}' for property_id in property_ids)\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?jstorID ?item ?itemLabel { ' '.join(f'?{property_id}' for property_id in property_ids) } ?article WHERE {{\n",
    "      VALUES (?jstorID) {{ {values_clause} }}\n",
    "      ?item wdt:P1230 ?jstorID.\n",
    "      {where_clause}\n",
    "      OPTIONAL {{\n",
    "        ?article schema:about ?item;\n",
    "                 schema:isPartOf <https://en.wikipedia.org/>.\n",
    "      }}\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return sparql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_fields_df = pd.read_csv(\"../data/original_journal_datasets/wiki/wikidata_fields.csv\")\n",
    "properties = wiki_data_fields_df['property_id'].tolist()\n",
    "sparql_query = create_batch_query(['amerhistrevi'], properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "def query_wikidata(sparql_query):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (compatible; YourTool/0.1; +http://YourWebSite.com/Bot)',\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params={'query': sparql_query})\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP error: {err}\")\n",
    "        time.sleep(10)  # Wait 10 seconds before retrying\n",
    "        return query_wikidata(sparql_query)  # Retry the query\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request error: {err}\")\n",
    "        time.sleep(10)  # Wait 10 seconds before retrying\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_wikidata(sparql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.json_normalize(results['results']['bindings'])\n",
    "columns_with_more_than_one_unique_value = test_results.columns[test_results.nunique() > 1].tolist()\n",
    "test_results[columns_with_more_than_one_unique_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_columns = test_results.columns.tolist()\n",
    "type_columns = [x for x in type_columns if '.type' in x]\n",
    "\n",
    "uri_columns = []\n",
    "for column in type_columns:\n",
    "    rows = test_results[test_results[column] == \"uri\"]\n",
    "    if len(rows) > 0:\n",
    "        uri_columns.append(column)\n",
    "uri_columns\n",
    "value_columns = [x.replace('.type', '.value') for x in uri_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[uri_columns + value_columns].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
